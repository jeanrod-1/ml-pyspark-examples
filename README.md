Machine Learning con PySpark en Cluster HPC

Este repositorio contiene una serie de notebooks dise√±ados para demostrar t√©cnicas de procesamiento de datos y Machine Learning utilizando Apache Spark sobre un entorno de computaci√≥n de alto desempe√±o (HPC).
El objetivo es mostrar flujos de trabajo reales y escalables para an√°lisis y modelado en grandes vol√∫menes de datos.

Contenido del repositorio
üîπ 1. MLlib ‚Äì No Supervisado

Notebook dedicado a t√©cnicas de aprendizaje no supervisado usando PySpark MLlib.
Incluye:

Limpieza y preparaci√≥n de datos

Escalamiento y ensamblaje de features

Modelado con K-means

Evaluaci√≥n con Silhouette Score y Elbow Method

Visualizaci√≥n e interpretaci√≥n de clusters

üîπ 2. MLlib ‚Äì Supervisado

Notebook centrado en algoritmos supervisados en Spark.
Incluye:

Preparaci√≥n y particionado de datos

Entrenamiento con regresi√≥n

M√©tricas de evaluaci√≥n (RMSE, accuracy, AUC)

Optimizaci√≥n de hiperpar√°metros

Predicci√≥n y an√°lisis de resultados

üîπ 3. Procesamiento y Transformaciones en Spark

Notebook enfocado en manipulaci√≥n y procesamiento distribuido:

Lectura y escritura de datos en formatos variados

Transformaciones con DataFrames

funciones avanzadas

Uso eficiente de recursos en un cluster HPC
