{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Maestría: Computación de Alto Desempeño\n",
    "\n",
    "##### Autor: **Jean Paul Rodríguez**\n",
    "##### Fecha: **18 de noviembre 2025**\n",
    "##### Tema: **Técnias de Machine Learning Supervisado con el ecosistema Apache Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b427e556-0872-4dcc-9f0b-6739bd5011c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Procesamiento de Datos a Gran Escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3d35310-cd72-43f7-92f0-0aa308209e2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<p><strong>Objetivo: </strong> El objetivo de este cuaderno es crear un flujo sencillo de machine learning</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se recorre paso a paso el flujo típico de un proyecto de Machine Learning supervisado usando Spark:\n",
    "\n",
    "1. Crear una sesión de Spark.\n",
    "2. Cargar el dataset.\n",
    "3. Hacer transformaciones, incluyendo vectorización de características y creación de interacciones.\n",
    "4. Entrenar un modelo de Regresión Logística Multinomial.\n",
    "5. Evaluarlo.\n",
    "6. Construir una Pipeline y usar Train/Validation Split con búsqueda de hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bfa4f5b5-6254-4238-9c8d-9e911a6eda34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cargar de datos en un Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio se estará utilizando el conjunto de datos Iris, la cual es una fuente en línea en formato CSV (valores separados por coma).\n",
    "\n",
    "<p> Este set de datos posee diferentes medidas sobre la planta Iris y es famosamente utilizado como ejemplo en analítica de datos:\n",
    "  </p>\n",
    "Se utiliza este conjunto para ejemplificar la creación de clusters:\n",
    "\n",
    "<ul>\n",
    "  <li>descripción: <a href=\"https://archive.ics.uci.edu/ml/datasets/Iris\" target=\"_blank\">https://archive.ics.uci.edu/ml/datasets/Iris</a></li>\n",
    "  <li>fuente de datos: <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" target=\"_blank\">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a></li>\n",
    "    <li>tipo de datos: csv</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b492c75-e3c1-427c-b0dc-3b4b918727d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En este ejemplo trabajaremos con el famoso dataset **Iris**, ampliamente utilizado en Machine\n",
    "Learning. Este dataset incluye:\n",
    "\n",
    "- 4 características numéricas:\n",
    "  - `sepal_length`\n",
    "  - `sepal_width`\n",
    "  - `petal_length`\n",
    "  - `petal_width`\n",
    "- Una columna `target` que representa la especie de la flor codificada como:\n",
    "  - 0 = setosa  \n",
    "  - 1 = versicolor  \n",
    "  - 2 = virginica  \n",
    "\n",
    "Cargaremos este dataset desde un archivo Parquet directamente en un DataFrame Spark,\n",
    "lo cual permite manejarlo de manera distribuida y aplicar transformaciones posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando la sesión de Spark\n",
    "\n",
    "Aquí configuramos Spark para que corra en modo FAIR scheduling y definimos cuántos cores\n",
    "y cuánta memoria queremos usar \n",
    "\n",
    "Finalmente construimos la SparkSession que será la base de todo nuestro trabajo en MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/17 19:06:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cadhead01.javeriana.edu.co:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.43.100.119:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hpcspark_jean</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbf4d49a070>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear sesión \n",
    "\n",
    "config = (\n",
    "    SparkConf()\n",
    "        .set(\"spark.scheduler.mode\", \"FAIR\")\n",
    "        .set(\"spark.executor.cores\", \"1\")\n",
    "        .set(\"spark.executor.memory\", \"4g\")\n",
    "        .set(\"spark.cores.max\", \"4\")\n",
    "        #.setMaster(\"spark://10.43.100.119:8080\")\n",
    "        .setMaster(\"spark://10.43.100.119:7077\")\n",
    "    )\n",
    "config.setAppName(\"hpcspark_jean\")\n",
    "spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "\n",
    "SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "contextoSpark = spark.sparkContext.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando el dataset Iris\n",
    "\n",
    "Leemos el archivo `iris.parquet` que contiene las 4 características numéricas del dataset Iris\n",
    "junto con la columna `target`, que representa la especie en formato numérico (0, 1 o 2).\n",
    "\n",
    "Luego mostramos las primeras filas para verificar que todo se haya cargado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# cargar iris con pandas\n",
    "iris = load_iris()\n",
    "pdf = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "pdf[\"target\"] = iris.target\n",
    "\n",
    "# convertir a Spark\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# guardar como parquet usando Spark (NO necesita pyarrow)\n",
    "df.write.mode(\"overwrite\").parquet(\"iris.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8a4e957-88ef-457f-8946-f2e56e04cfdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|              4.9|             3.6|              1.4|             0.1|     0|\n",
      "|              4.4|             3.0|              1.3|             0.2|     0|\n",
      "|              5.1|             3.4|              1.5|             0.2|     0|\n",
      "|              5.0|             3.5|              1.3|             0.3|     0|\n",
      "|              4.5|             2.3|              1.3|             0.3|     0|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"iris.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renombrando las columnas\n",
    "\n",
    "Las columnas originales tienen espacios y paréntesis, lo cual es molesto cuando usamos\n",
    "RFormula o escribimos transformaciones.\n",
    "\n",
    "Así que renombramos cada columna a un formato más amigable en Spark: `sepal_length`,\n",
    "`sepal_width`, `petal_length`, `petal_width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         4.9|        3.6|         1.4|        0.1|     0|\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|\n",
      "|         5.1|        3.4|         1.5|        0.2|     0|\n",
      "|         5.0|        3.5|         1.3|        0.3|     0|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df \\\n",
    "    .withColumnRenamed(\"sepal length (cm)\", \"sepal_length\") \\\n",
    "    .withColumnRenamed(\"sepal width (cm)\", \"sepal_width\") \\\n",
    "    .withColumnRenamed(\"petal length (cm)\", \"petal_length\") \\\n",
    "    .withColumnRenamed(\"petal width (cm)\", \"petal_width\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ead1a57c-db9f-43c8-9289-34599a0f709f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cfe71ef1-9c01-426e-8474-01619db854d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El conjunto de datos actual no cumple con el requisito de estar en formato de Vector y, por lo tanto, debemos transformarlo al formato adecuado.\n",
    "\n",
    "Para lograr esto en nuestro ejemplo, vamos a especificar una RFormula. Este es un lenguaje declarativo para especificar transformaciones de aprendizaje automático y es fácil de usar una vez que comprende la sintaxis.\n",
    "\n",
    "Los operadores básicos de RFormula son:\n",
    "<p>\n",
    "<p>\"~\" Destino y términos separados</p>\n",
    "<p>\"+\" Términos de Concat; \"+ 0\" significa eliminar la intersección (esto significa que la intersección y de la línea que ajustaremos será 0)</p>\n",
    "<p>\"-\" Eliminar un término; \"- 1\" significa eliminar la intersección (esto significa que la intersección y de la línea que vamos a ajustar será 0; sí, esto hace lo mismo que \"+ 0\"</p>\n",
    "<p>\":\" Interacción (multiplicación de valores numéricos o valores categóricos binarizados)</p>\n",
    "<p>\".\" Todas las columnas excepto la variable objetivo / dependiente</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf996bfa-9c00-42e6-b38c-3198c937235c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para especificar transformaciones con esta sintaxis, necesitamos importar la clase RFormula. \n",
    "\n",
    "\n",
    "Para entrenar modelos en Spark MLlib, las variables deben estar unificadas dentro de una sola\n",
    "columna llamada `features`, que contiene un vector con las características numéricas.\n",
    "\n",
    "En el caso del dataset **Iris**, todas las columnas predictoras son numéricas, así que no es necesario\n",
    "convertir categorías ni aplicar codificaciones especiales.\n",
    "\n",
    "Usaremos `RFormula`, que facilita:\n",
    "- Seleccionar el target (`target`)\n",
    "- Seleccionar todas las características (`.`)\n",
    "- Crear nuevas características basadas en interacciones entre variables numéricas\n",
    "\n",
    "En este notebook agregamos una interacción entre `sepal_length` y `petal_width`, creando\n",
    "una característica sintética que puede capturar relaciones más complejas entre dimensiones de\n",
    "la flor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a6a603a3-7960-464b-bfd0-efb86c6d5d31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(\n",
    "    formula=\"target ~ . + sepal_length:petal_width\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29547c28-244f-4b87-94a3-39d56c5a8d14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El siguiente paso es ajustar el transformador RFormula a los datos para que descubra los posibles valores de cada columna.\n",
    "\n",
    "No todos los transformadores tienen este requisito, pero debido a que RFormula manejará automáticamente las variables categóricas por nosotros, necesita determinar qué columnas son categóricas y cuáles no, así como cuáles son los valores distintos de las columnas categóricas.\n",
    "\n",
    "Por esta razón, tenemos que llamar al método fit. Una vez que llamamos a fit, devuelve una versión \"entrenada\" de nuestro transformador que luego podemos usar para transformar nuestros datos.\n",
    "\n",
    "Luego llamamos a transform en ese objeto para transformar nuestros datos de entrada en los datos de salida esperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11077b04-a475-4e39-bcf6-38836b6fc473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+--------------------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|            features|label|\n",
      "+------------+-----------+------------+-----------+------+--------------------+-----+\n",
      "|         4.9|        3.6|         1.4|        0.1|     0|[4.9,3.6,1.4,0.1,...|  0.0|\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2,...|  0.0|\n",
      "|         5.1|        3.4|         1.5|        0.2|     0|[5.1,3.4,1.5,0.2,...|  0.0|\n",
      "|         5.0|        3.5|         1.3|        0.3|     0|[5.0,3.5,1.3,0.3,...|  0.0|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|[4.5,2.3,1.3,0.3,...|  0.0|\n",
      "|         4.4|        3.2|         1.3|        0.2|     0|[4.4,3.2,1.3,0.2,...|  0.0|\n",
      "|         5.0|        3.5|         1.6|        0.6|     0|[5.0,3.5,1.6,0.6,...|  0.0|\n",
      "|         5.1|        3.8|         1.9|        0.4|     0|[5.1,3.8,1.9,0.4,...|  0.0|\n",
      "|         4.8|        3.0|         1.4|        0.3|     0|[4.8,3.0,1.4,0.3,...|  0.0|\n",
      "|         5.1|        3.8|         1.6|        0.2|     0|[5.1,3.8,1.6,0.2,...|  0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2,...|  0.0|\n",
      "|         5.3|        3.7|         1.5|        0.2|     0|[5.3,3.7,1.5,0.2,...|  0.0|\n",
      "|         5.0|        3.3|         1.4|        0.2|     0|[5.0,3.3,1.4,0.2,...|  0.0|\n",
      "|         7.0|        3.2|         4.7|        1.4|     1|[7.0,3.2,4.7,1.4,...|  1.0|\n",
      "|         6.4|        3.2|         4.5|        1.5|     1|[6.4,3.2,4.5,1.5,...|  1.0|\n",
      "|         6.9|        3.1|         4.9|        1.5|     1|[6.9,3.1,4.9,1.5,...|  1.0|\n",
      "|         5.5|        2.3|         4.0|        1.3|     1|[5.5,2.3,4.0,1.3,...|  1.0|\n",
      "|         6.5|        2.8|         4.6|        1.5|     1|[6.5,2.8,4.6,1.5,...|  1.0|\n",
      "|         5.7|        2.8|         4.5|        1.3|     1|[5.7,2.8,4.5,1.3,...|  1.0|\n",
      "|         6.3|        3.3|         4.7|        1.6|     1|[6.3,3.3,4.7,1.6,...|  1.0|\n",
      "+------------+-----------+------------+-----------+------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "fittedRF = supervised.fit(df) # Ajusta\n",
    "preparedDF = fittedRF.transform(df) # Transforma\n",
    "preparedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6bc2ae8f-7b6f-4c6e-90d9-753437ebcc18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En la salida podemos ver el resultado final: Spark creó las columnas `features` y `label` a partir de\n",
    "la fórmula definida.\n",
    "\n",
    "Dado que Iris solo contiene valores numéricos, `RFormula` simplemente:\n",
    "- Agrupa las columnas numéricas en un único vector (`features`)\n",
    "- Garantiza que `target` quede en formato numérico apto para modelado (`label`)\n",
    "- Aplica la interacción especificada entre `sepal_length` y `petal_width`\n",
    "\n",
    "Este proceso deja los datos listos para entrenar un modelo de clasificación multinomial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c11c50e-fa58-4af3-b957-c4fbd2030721",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creemos ahora un conjunto de prueba simple basado en una división aleatoria de los datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División Train/Test\n",
    "\n",
    "Dividimos los datos usando una separación de 70/30 para poder entrenar el modelo y evaluar\n",
    "su rendimiento en datos que no ha visto antes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ecb0e02-bcd5-4957-ba7a-fa689a88d32e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a09fe42-8434-4a1a-9635-8a009c30f51c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d287b901-ac74-48c7-80ea-d62ee69c1a70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El modelo que utilizaremos es la **Regresión Logística Multinomial**, una extensión natural de la\n",
    "regresión logística clásica para problemas donde existen más de dos clases.  \n",
    "En el dataset Iris tenemos **tres especies distintas**, por lo que Spark automáticamente activa\n",
    "el modo multinomial.\n",
    "\n",
    "Creamos una instancia de `LogisticRegression`, indicando qué columna contiene las etiquetas\n",
    "(`target`) y qué columna contiene el vector de características (`features`).  \n",
    "Luego mostramos los parámetros disponibles para entender qué opciones se pueden ajustar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f829b0a-39d3-449e-97e8-bddc44904cc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"target\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7793a8bf-ecf2-4070-b522-1e83c2f12b07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Antes de comenzar a entrenar este modelo, inspeccionemos los parámetros.\n",
    "\n",
    "Este método muestra una explicación de todos los parámetros para la implementación de Spark de la regresión logística.\n",
    "\n",
    "El método \"explainParams\" existe en todos los algoritmos disponibles en MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "900fa03a-7bdc-4175-8091-257a9bcdf90d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: target)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando el modelo\n",
    "\n",
    "Al crear una instancia de un algoritmo no entrenado, llega el momento de ajustarlo a los datos (entrenarlo). En este caso, esto devuelve un LogisticRegressionModel.\n",
    "\n",
    "Este código iniciará un trabajo de Spark para entrenar el modelo. A diferencia de las transformaciones, el ajuste de un modelo de aprendizaje automático es ansioso y se realiza de inmediato.\n",
    "\n",
    "Entrenamos el modelo con los datos de entrenamiento.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab7501ad-ed35-4d1a-ade0-9b9a5eb47100",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "534ade73-8c65-45e0-9ba1-7b33e2fa7009",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Predicciones sobre el conjunto de entrenamiento\n",
    "\n",
    "Utilizamos el modelo entrenado para generar predicciones sobre los datos de `train` y\n",
    "visualizamos las columnas `target` y `prediction`.  \n",
    "Esto nos permite ver si el modelo está clasificando correctamente cada punto.\n",
    "\n",
    "Una vez completado, puede usar el modelo para hacer predicciones. Lógicamente, esto significa transformar características en etiquetas.\n",
    "\n",
    "Hacemos predicciones con el método transform. Por ejemplo, podemos transformar nuestro conjunto de datos de entrenamiento para ver qué etiquetas asignó nuestro modelo a los datos de entrenamiento y cómo se comparan con los resultados reales.\n",
    "\n",
    "Realicemos esa predicción con el siguiente fragmento de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ee5de9e5-cfdd-4e9f-9470-63aa116dc1a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|target|prediction|\n",
      "+------+----------+\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     2|       2.0|\n",
      "|     2|       2.0|\n",
      "|     1|       1.0|\n",
      "|     2|       2.0|\n",
      "|     2|       2.0|\n",
      "|     1|       1.0|\n",
      "|     2|       2.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fittedLR.transform(train).select(\"target\", \"prediction\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al transformar el conjunto de entrenamiento podemos ver la predicción que el modelo asigna a\n",
    "cada fila. En este caso, las clases posibles son 0, 1 y 2, correspondientes a las tres especies de\n",
    "Iris.  \n",
    "Esto nos permite identificar rápidamente si el modelo está clasificando correctamente los\n",
    "ejemplos dentro de cada clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "197bad2a-9e39-42b8-92ab-cd0e0dc4f4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nuestro siguiente paso sería evaluar manualmente este modelo y calcular métricas de rendimiento como la tasa de verdaderos positivos, la tasa de falsos negativos, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ef5cc685-f7c7-41af-af98-38d74227b3aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08a020c2-f342-4837-ad49-63af3c567c61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Cuando un flujo de Machine Learning incluye varios pasos encadenados (transformación de\n",
    "datos, vectorización, modelado, etc.), puede volverse difícil mantener el seguimiento de cada\n",
    "DataFrame intermedio.\n",
    "\n",
    "Para evitar escribir cada paso de forma manual, usamos `Pipeline`.  \n",
    "\n",
    "Spark ML proporciona `Pipeline` para agrupar todas las etapas en un solo objeto.  \n",
    "En nuestro caso, la Pipeline tendrá dos stages:\n",
    "\n",
    "1. `RFormula` → crea el vector de características y la interacción\n",
    "2. `LogisticRegression` → entrena el modelo multinomial\n",
    "\n",
    "Al ejecutar la Pipeline, Spark aplica todas las stages en secuencia y devuelve un modelo final\n",
    "ya entrenado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "149a5c9b-b83e-4cf8-ae34-63bab1c68fb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec7dcb71-dd5d-4540-8066-87e83acd4c58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora que tiene un conjunto de entrenamiento y prueba, creemos las stages base en nuestra Pipeline.\n",
    "\n",
    "Una stage simplemente representa un transformador o un estimador. En nuestro caso, tendremos dos estimadores. La RFomula y el LogisticRegresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed582fda-ecc0-4be8-8301-969ffe5bef2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"target\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "02559619-d928-4166-a14a-d09cecd0d419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora, en lugar de usar manualmente nuestras transformaciones y luego ajustar nuestro modelo, simplemente las hacemos stages en la Pipeline general, como en el siguiente fragmento de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "748b2700-bcc7-4ba4-87e8-cad9d0cc542b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "212be521-a72b-4dc9-b4f5-2e634eb02a80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Entrenamiento y Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "281349c1-adcd-4ab0-b73d-bc3af76598a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora que organizó la Pipeline, el siguiente paso es el Entrenamiento.\n",
    "\n",
    "En este caso, no entrenaremos solo un modelo. Entrenaremos varias variaciones del modelo especificando diferentes combinaciones de hiperparámetros que nos gustaría que Spark probara.\n",
    "\n",
    "Luego, seleccionaremos el mejor modelo usando un evaluador que compara sus predicciones con nuestros datos de validación.\n",
    "\n",
    "Podemos probar diferentes hiperparámetros en toda la Pipeline, incluso en la fórmula de RF que usamos para manipular los datos sin procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección definimos una grilla de hiperparámetros para explorar diferentes configuraciones\n",
    "del modelo y de la fórmula.  \n",
    "Incluimos:\n",
    "\n",
    "- Dos variantes de la fórmula (una con una interacción extra)\n",
    "- Tres valores para `elasticNetParam`, que controla la mezcla entre L1 y L2\n",
    "- Dos valores para `regParam`, que controla la regularización\n",
    "\n",
    "Esto genera 12 combinaciones diferentes que serán probadas durante la validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e78f15e-9c00-4bdc-82c7-335affe06059",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder()\\\n",
    ".addGrid(rForm.formula, [\n",
    "\"target ~ . + sepal_length:petal_width\",\n",
    "\"target ~ . + sepal_length:petal_width + sepal_length:petal_length\"])\\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    ".addGrid(lr.regParam, [0.1, 2.0])\\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcfec495-6772-478e-b31f-bece00f29fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Evaluación del modelo\n",
    "\n",
    "Ahora que la cuadrícula está construida, es hora de especificar nuestro proceso de evaluación. El evaluador nos permite comparar de forma automática y objetiva varios modelos con la misma métrica de evaluación.\n",
    "\n",
    "Para comparar el rendimiento de cada combinación de hiperparámetros, utilizamos\n",
    "`MulticlassClassificationEvaluator` con la métrica `f1` dado que tenemos un problema de clasificación multiclase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "273cd2d9-cf7d-4b15-8d40-1eb213564d94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora que tenemos una canalización que especifica cómo se deben transformar nuestros datos, realizaremos la selección del modelo para probar diferentes hiperparámetros en nuestro modelo de regresión logística y medir el éxito comparando su desempeño usando la métrica areaUnderROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y Evaluación\n",
    "\n",
    "Usamos `TrainValidationSplit` para entrenar varias variantes del modelo y seleccionar aquella que\n",
    "obtiene el mejor desempeño según la métrica especificada.  \n",
    "\n",
    "- Entrena cada modelo con 75% del train\n",
    "- Valida con 25% del train\n",
    "- Elige el mejor\n",
    "\n",
    "Finalmente evaluamos el modelo ganador en el conjunto de prueba (`test`), para ver cómo se\n",
    "comporta con datos completamente nuevos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e35b9a12-0c9e-40aa-84b1-2ce640bf5e8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit()\\\n",
    ".setTrainRatio(0.75)\\\n",
    ".setEstimatorParamMaps(params)\\\n",
    ".setEstimator(pipeline)\\\n",
    ".setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bcd3e4d1-19ef-455a-8f6d-eb24ebeba5f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ejecutemos toda la Pipeline que construimos. Para revisar, la ejecución de esta canalización probará todas las versiones del modelo con el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84857ba8-1742-47a8-95ec-82909d1969f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f78f9969-e788-40a3-9185-745075fa7cd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Tambien se evalua cómo funciona el algoritmo con el conjunto de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab357264-cbf0-4287-801f-3cb3c5ada7df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466208133971292"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen y Conclusiones\n",
    "\n",
    "En este notebook se recorre el proceso completo de Machine Learning Supervisado en Spark MLlib usando el\n",
    "dataset Iris:\n",
    "\n",
    "- Cargamos datos y preprocesamos las columnas.\n",
    "- Creamos transformaciones usando RFormula, incluyendo una interacción entre variables.\n",
    "- Entrenamos un modelo de Regresión Logística Multinomial.\n",
    "- Evaluamos sus predicciones.\n",
    "- Construimos una Pipeline completa.\n",
    "- Ejecutamos una búsqueda de hiperparámetros con TrainValidationSplit.\n",
    "- Obtuvimos el mejor modelo y lo evaluamos con métricas estándar.\n",
    "\n",
    "Este flujo es la base de cómo Spark maneja ML a escala y permite extenderlo fácilmente a\n",
    "datasets mucho más grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01 - ML_EjemploInicial",
   "notebookOrigID": 1906006966565397,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
