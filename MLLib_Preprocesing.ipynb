{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maestría: Computación de Alto Desempeño\n",
    "\n",
    "##### Autor: **Jean Paul Rodríguez**\n",
    "##### Fecha: **18 de noviembre 2025**\n",
    "##### Tema: **Procesamiento de Alto Volumen de Datos con el ecosistema Apache Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1c3133a-528c-415a-99ec-d3f6eec391e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Procesamiento de Datos a Gran Escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "39597e5c-16c3-4488-bc69-5743cb40b8ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<p><strong>Objetivo: </strong> El objetivo de este cuaderno es aprender sentencias pyspark para el preprocesamiento de los datos:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se hace un recorrido por las técnicas y procesamientos más comunes que uno hace cuando trabaja con datos en Spark: limpiar, arreglar, transformar y normalizar.\n",
    "\n",
    "Se muestran jemplos simples pero que sirven igual con datasets gigantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrancando Spark\n",
    "\n",
    "En esta parte simplemente encendemos Spark con una configuración personalizada\n",
    "\n",
    "cuántos cores usar, cuánta memoria, y a qué master conectarnos\n",
    "\n",
    "Se deja listo el entorno para empezar a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/17 16:09:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/17 16:09:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cadhead01.javeriana.edu.co:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.43.100.119:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hpcspark_jean</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efc314fe5b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear sesión \n",
    "\n",
    "config = (\n",
    "    SparkConf()\n",
    "        .set(\"spark.scheduler.mode\", \"FAIR\")\n",
    "        .set(\"spark.executor.cores\", \"1\")\n",
    "        .set(\"spark.executor.memory\", \"4g\")\n",
    "        .set(\"spark.cores.max\", \"4\")\n",
    "        #.setMaster(\"spark://10.43.100.119:8080\")\n",
    "        .setMaster(\"spark://10.43.100.119:7077\")\n",
    "    )\n",
    "config.setAppName(\"hpcspark_jean\")\n",
    "spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "\n",
    "SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "contextoSpark = spark.sparkContext.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1681c34-5df0-4581-a5a6-b0a0f1faaa5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Identificación y tratamiento de valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un DataFrame con valores faltantes\n",
    "\n",
    "Aquí armamos un DataFrame con varios None metidos por ahí\n",
    "\n",
    "Se usará para practicar cómo detectar y tratar nulos sin usar un dataset real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d611ed7-a448-4e0e-a0ca-7cbcdc3cec5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "[\n",
    "('Store 1',1,448),\n",
    "('Store 1',2,None),\n",
    "('Store 1',3,499),\n",
    "('Store 1',44,432),\n",
    "(None,None,None),\n",
    "('Store 2',1,355),\n",
    "('Store 2',1,355),\n",
    "('Store 2',None,345),\n",
    "('Store 2',3,387),\n",
    "('Store 2',4,312),\n",
    "],\n",
    "['Store','WeekInMonth','Revenue']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fda51bf6-4e4a-44f9-ac2e-77630bb5f8d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Indentificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando valores nulos\n",
    "\n",
    "Primero miramos qué filas tienen Revenue = NULL\n",
    "\n",
    "Luego hacemos un conteo por cada columna para saber cuántos nulos tiene cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "07364fed-fc84-48e3-8249-7f3266ae5238",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          2|   NULL|\n",
      "|   NULL|       NULL|   NULL|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Revenue.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contando nulos por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b0d02e27-6ad7-482a-868b-ce25377e8ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------+\n",
      "|Store|WeekInMonth|Revenue|\n",
      "+-----+-----------+-------+\n",
      "|    1|          2|      2|\n",
      "+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when, isnull\n",
    "\n",
    "df.select(\n",
    "[count(when(isnull(c), c)).alias(c) for c in df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29b88921-610b-4302-8df8-31191a18b757",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Eliminado registros con valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando filas con nulos\n",
    "\n",
    "Probamos dos formas distintas:\n",
    "\n",
    "dropna() → elimina filas apenas tengan un null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9cccba49-7426-43cb-98b3-1a24e449c10b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.dropna()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropna('all') → solo borra filas que estén completamente vacías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84a376b6-4fd9-4436-8988-954f69a9d833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|   NULL|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.dropna('all')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b388391-419b-4d5a-86c6-d491ba78833a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There is one important thing to note about fillna – it’ll only do the exchange\n",
    "operation for matching column types. So if you use a numeric value for a string column\n",
    "or the other way around, it won’t work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0674787d-4295-4dac-9474-06475ef37fa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sustituyendo por un valor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rellenando valores faltantes (fillna)\n",
    "\n",
    "Acá empezamos a rellenar los valores faltantes\n",
    "\n",
    "Rellenar con 0 en todas las columnas\n",
    "\n",
    "Rellenar solo en columnas específicas\n",
    "\n",
    "Rellenar cada columna con un valor distinto usando un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e531980a-4f6a-4393-a405-a4ea00ed572e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|      0|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|          0|      0|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          0|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n",
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|      0|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|       NULL|      0|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n",
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|      3|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|          2|      3|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          2|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0).show()\n",
    "df.fillna(0, ['Revenue']).show()\n",
    "df.fillna({'WeekInMonth' : 2, 'Revenue' : 3}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ceab568b-18bb-4ca1-b58d-fceab6545737",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sustituyendo con la media\n",
    "\n",
    "Se calcula la media de la columna Revenue y se usa ese valor para hacer el fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6948ad6f-f564-463e-88ad-2a971bb6299f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|avg(Revenue)|\n",
      "+------------+\n",
      "|     391.625|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(df.Revenue)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "727fe983-f268-4061-9a1e-b8c0e7ea1fd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|    391|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|       NULL|    391|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(391.625, ['Revenue']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8dd26dd5-f5ee-49e7-b475-edf25515f00a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Eliminando duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos dos ejemplos:\n",
    "\n",
    "Quitar duplicados completos de la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35b8ff04-02b5-4385-a640-24f5ad35a312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 2|          4|    312|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          3|    387|\n",
      "|Store 1|         44|    432|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|          1|    448|\n",
      "|Store 2|       NULL|    345|\n",
      "|   NULL|       NULL|   NULL|\n",
      "|Store 1|          2|   NULL|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitar duplicados basados solo en ciertas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4dcd33e6-e843-406a-8c75-4706aed56b5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          4|    312|\n",
      "|Store 1|          2|   NULL|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|          1|    448|\n",
      "|   NULL|       NULL|   NULL|\n",
      "|Store 1|         44|    432|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['Store','WeekInMonth']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8948a99d-ea63-403b-be7f-ddbd44772485",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Eliminando columnas\n",
    "\n",
    "Acá literalmente borramos columnas que no se necesitan\n",
    "\n",
    "Se pueden borrar una o varias columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cea25f65-0fe1-4a23-84c5-68503714fd68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|  Store|WeekInMonth|\n",
      "+-------+-----------+\n",
      "|Store 1|          1|\n",
      "|Store 1|          2|\n",
      "|Store 1|          3|\n",
      "|Store 1|         44|\n",
      "|   NULL|       NULL|\n",
      "|Store 2|          1|\n",
      "|Store 2|          1|\n",
      "|Store 2|       NULL|\n",
      "|Store 2|          3|\n",
      "|Store 2|          4|\n",
      "+-------+-----------+\n",
      "\n",
      "+-----------+\n",
      "|WeekInMonth|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          2|\n",
      "|          3|\n",
      "|         44|\n",
      "|       NULL|\n",
      "|          1|\n",
      "|          1|\n",
      "|       NULL|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('Revenue').show()\n",
    "df.drop('Revenue','Store').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c14c5271-b853-4970-a6da-4511a480f437",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Identificando y resolviendo valores inconsistentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploramos el df, se hace un show para ver sus columnas y sus filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e8525a8-ab70-4e3a-b0b1-803c92999d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|   NULL|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|       NULL|   NULL|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos describe() para ver stats como media, mínima, máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "750e400a-6c65-40eb-b30c-ef866869bf14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:======================================>                   (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+-----------------+\n",
      "|summary|  Store|      WeekInMonth|          Revenue|\n",
      "+-------+-------+-----------------+-----------------+\n",
      "|  count|      4|                4|                3|\n",
      "|   mean|   NULL|             12.5|459.6666666666667|\n",
      "| stddev|   NULL|21.01586702153082|34.99047489436709|\n",
      "|    min|Store 1|                1|              432|\n",
      "|    max|Store 1|               44|              499|\n",
      "+-------+-------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.filter(df.Store == 'Store 1').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e7bac51-328e-44b4-81e7-8a7735b349b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Esto dará el valor en un cuantil dado, en el intervalo de 0 a 1. Por lo tanto, si establece el segundo argumento en 0.0, obtendrá el valor más bajo para la columna. Con 1.0 obtienes el valor más alto. En el medio tienes la mediana, que es lo que se está buscando:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego usamos approxQuantile() para sacar valores como la mediana sin matar la memoria\n",
    "\n",
    "Esto nos ayuda a darnos una idea de si algo está fuera de lugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9af17542-a17d-462d-840e-22644d0a8394",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[355.0]\n"
     ]
    }
   ],
   "source": [
    "print(df.approxQuantile('Revenue', [0.5], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5b09724-8041-4b64-bfc5-0a881fcf1259",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "085ac321-043e-4e97-a78b-d3ce5f28c5e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A veces, desea cambiar sus datos de filas a columnas. La función se llama pivotar y está disponible en Pyspark.\n",
    "\n",
    "Básicamente, estás rotando los datos alrededor de un eje determinado, de ahí el nombre.\n",
    "\n",
    "En este caso, ese eje son los datos en una de sus columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "934de8c1-43cc-490d-ae25-3c110b8c9156",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-------+-------+\n",
      "|WeekInMonth|null|Store 1|Store 2|\n",
      "+-----------+----+-------+-------+\n",
      "|       NULL|NULL|   NULL|    345|\n",
      "|          1|NULL|    448|    710|\n",
      "|          2|NULL|   NULL|   NULL|\n",
      "|          3|NULL|    499|    387|\n",
      "|          4|NULL|   NULL|    312|\n",
      "|         44|NULL|    432|   NULL|\n",
      "+-----------+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivoted = df.groupBy('WeekInMonth').pivot('Store').sum('Revenue').orderBy('WeekInMonth')\n",
    "df_pivoted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El pivot es muy útil cuando se quiere ver métricas por categoría (por ejemplo, por tienda)\n",
    "\n",
    "Acá se agrupa por store y semana y se hace la suma de revenue para esos grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e79fcc9-6f8e-44ea-9f2b-9767dfd0aab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+\n",
      "|  Store|WeekInMonth|sum(Revenue)|\n",
      "+-------+-----------+------------+\n",
      "|Store 2|       NULL|         345|\n",
      "|   NULL|       NULL|        NULL|\n",
      "|Store 2|          1|         710|\n",
      "|Store 1|          1|         448|\n",
      "|Store 1|          2|        NULL|\n",
      "|Store 2|          3|         387|\n",
      "|Store 1|          3|         499|\n",
      "|Store 2|          4|         312|\n",
      "|Store 1|         44|         432|\n",
      "+-------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    ".groupBy('Store','WeekInMonth')\n",
    ".sum('Revenue')\n",
    ".orderBy('WeekInMonth')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá deshacemos el pivot usando stack, así nos queda el df como se tenía antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18c1f454-e53c-42e5-8847-4f55064e879f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+\n",
      "|WeekInMonth|  Store|Revenue|\n",
      "+-----------+-------+-------+\n",
      "|       NULL|Store 1|   NULL|\n",
      "|       NULL|Store 2|    345|\n",
      "|          1|Store 1|    448|\n",
      "|          1|Store 2|    710|\n",
      "|          2|Store 1|   NULL|\n",
      "|          2|Store 2|   NULL|\n",
      "|          3|Store 1|    499|\n",
      "|          3|Store 2|    387|\n",
      "|          4|Store 1|   NULL|\n",
      "|          4|Store 2|    312|\n",
      "|         44|Store 1|    432|\n",
      "|         44|Store 2|   NULL|\n",
      "+-----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_pivoted.withColumnRenamed('Store 1','Store1')\n",
    "        .withColumnRenamed('Store 2','Store2')\n",
    "        .selectExpr('WeekInMonth',\"stack(2, 'Store 1', Store1, 'Store 2', Store2) as (Store,Revenue)\").show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "95c34bcd-6359-4123-9f7f-64743e8cc10c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af3e013d-a5e1-466d-b2ea-0ac5fa39f657",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hay otra situación con la que te encontrarás de vez en cuando. A veces llegan varios puntos de datos juntos en una columna. Esto usual cuando JSON es el formato de origen.\n",
    "\n",
    "Puede resolver este problema utilizando el comando de Explode. Tomará la cadena con varios valores y los colocará en una fila cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be8d8cb4-d34e-4e3e-b285-31fb049f2007",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|watches|\n",
      "+---+-------+\n",
      "|  1|  Rolex|\n",
      "|  1|  Patek|\n",
      "|  1| Jaeger|\n",
      "|  2|  Omega|\n",
      "|  2|  Heuer|\n",
      "|  3| Swatch|\n",
      "|  3|  Rolex|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(1, ['Rolex','Patek','Jaeger']),\n",
    "(2, ['Omega','Heuer']),\n",
    "(3, ['Swatch','Rolex'])],\n",
    "('id','watches'))\n",
    "(df.withColumn('watches',explode(df.watches))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c2feb94-5722-4096-86c1-149f122b892f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio se estará utilizando el conjunto de datos Iris, la cual es una fuente en línea en formato CSV (valores separados por coma).\n",
    "\n",
    "<p> Este set de datos posee diferentes medidas sobre la planta Iris y es famosamente utilizado como ejemplo en analítica de datos:\n",
    "  </p>\n",
    "Se utiliza este conjunto para ejemplificar la creación de clusters:\n",
    "\n",
    "<ul>\n",
    "  <li>descripción: <a href=\"https://archive.ics.uci.edu/ml/datasets/Iris\" target=\"_blank\">https://archive.ics.uci.edu/ml/datasets/Iris</a></li>\n",
    "  <li>fuente de datos: <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" target=\"_blank\">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a></li>\n",
    "    <li>tipo de datos: csv</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/estudiante/jupyter_env/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/estudiante/jupyter_env/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/estudiante/jupyter_env/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/estudiante/jupyter_env/lib/python3.9/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/estudiante/jupyter_env/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# cargar iris con pandas\n",
    "iris = load_iris()\n",
    "pdf = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "pdf[\"target\"] = iris.target\n",
    "\n",
    "# convertir a Spark\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# guardar como parquet usando Spark (NO necesita pyarrow)\n",
    "df.write.mode(\"overwrite\").parquet(\"iris.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el df público de \"iris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|              4.9|             3.6|              1.4|             0.1|     0|\n",
      "|              4.4|             3.0|              1.3|             0.2|     0|\n",
      "|              5.1|             3.4|              1.5|             0.2|     0|\n",
      "|              5.0|             3.5|              1.3|             0.3|     0|\n",
      "|              4.5|             2.3|              1.3|             0.3|     0|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irisDF = spark.read.parquet(\"iris.parquet\")\n",
    "irisDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero convertimos las columnas numéricas en vector para hacer la normalización\n",
    "\n",
    "Luego la escalamos para que sus valores queden entre 5 y 10\n",
    "\n",
    "Esto deja los datos en un rango uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2b672089-f6c7-4d89-8766-6bdf01b94d8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+------+---------------+-------------------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|sepal_width_vec|sepal_width_scaled |\n",
      "+-----------------+----------------+-----------------+----------------+------+---------------+-------------------+\n",
      "|4.9              |3.6             |1.4              |0.1             |0     |[3.6]          |[8.333333333333332]|\n",
      "|4.4              |3.0             |1.3              |0.2             |0     |[3.0]          |[7.083333333333333]|\n",
      "|5.1              |3.4             |1.5              |0.2             |0     |[3.4]          |[7.916666666666666]|\n",
      "|5.0              |3.5             |1.3              |0.3             |0     |[3.5]          |[8.125]            |\n",
      "|4.5              |2.3             |1.3              |0.3             |0     |[2.3]          |[5.625]            |\n",
      "|4.4              |3.2             |1.3              |0.2             |0     |[3.2]          |[7.5]              |\n",
      "|5.0              |3.5             |1.6              |0.6             |0     |[3.5]          |[8.125]            |\n",
      "|5.1              |3.8             |1.9              |0.4             |0     |[3.8]          |[8.75]             |\n",
      "|4.8              |3.0             |1.4              |0.3             |0     |[3.0]          |[7.083333333333333]|\n",
      "|5.1              |3.8             |1.6              |0.2             |0     |[3.8]          |[8.75]             |\n",
      "|4.6              |3.2             |1.4              |0.2             |0     |[3.2]          |[7.5]              |\n",
      "|5.3              |3.7             |1.5              |0.2             |0     |[3.7]          |[8.541666666666666]|\n",
      "|5.0              |3.3             |1.4              |0.2             |0     |[3.3]          |[7.708333333333332]|\n",
      "|7.0              |3.2             |4.7              |1.4             |1     |[3.2]          |[7.5]              |\n",
      "|6.4              |3.2             |4.5              |1.5             |1     |[3.2]          |[7.5]              |\n",
      "|6.9              |3.1             |4.9              |1.5             |1     |[3.1]          |[7.291666666666666]|\n",
      "|5.5              |2.3             |4.0              |1.3             |1     |[2.3]          |[5.625]            |\n",
      "|6.5              |2.8             |4.6              |1.5             |1     |[2.8]          |[6.666666666666666]|\n",
      "|5.7              |2.8             |4.5              |1.3             |1     |[2.8]          |[6.666666666666666]|\n",
      "|6.3              |3.3             |4.7              |1.6             |1     |[3.3]          |[7.708333333333332]|\n",
      "+-----------------+----------------+-----------------+----------------+------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "\n",
    "# Convertir columna numérica a vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"sepal width (cm)\"],\n",
    "    outputCol=\"sepal_width_vec\"\n",
    ")\n",
    "\n",
    "iris_vec = assembler.transform(irisDF)\n",
    "\n",
    "# Escalar\n",
    "scaler = MinMaxScaler(\n",
    "    min=5,\n",
    "    max=10,\n",
    "    inputCol=\"sepal_width_vec\",\n",
    "    outputCol=\"sepal_width_scaled\"\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(iris_vec)\n",
    "scaled_df = scaler_model.transform(iris_vec)\n",
    "\n",
    "scaled_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora normalizamos pero usando media 0 y desviación estándar 1\n",
    "\n",
    "Este formato es muy común en machine learning\n",
    "especialmente cuando no se quiere que columnas con rangos grandes tengan más importancia que otras\n",
    "\n",
    "Esto nivela el campo para que la naturaleza y rango de las variables no impacte en su relevancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38844d32-9cf9-4841-b3bb-42028e1b89ba",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+------+---------------+-------------------+----------------------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|sepal_width_vec|sepal_width_scaled |sepal_width_std       |\n",
      "+-----------------+----------------+-----------------+----------------+------+---------------+-------------------+----------------------+\n",
      "|4.9              |3.6             |1.4              |0.1             |0     |[3.6]          |[8.333333333333332]|[1.2450301512664121]  |\n",
      "|4.4              |3.0             |1.3              |0.2             |0     |[3.0]          |[7.083333333333333]|[-0.13153881205026063]|\n",
      "|5.1              |3.4             |1.5              |0.2             |0     |[3.4]          |[7.916666666666666]|[0.7861738301608543]  |\n",
      "|5.0              |3.5             |1.3              |0.3             |0     |[3.5]          |[8.125]            |[1.0156019907136333]  |\n",
      "|4.5              |2.3             |1.3              |0.3             |0     |[2.3]          |[5.625]            |[-1.7375359359197124] |\n",
      "|4.4              |3.2             |1.3              |0.2             |0     |[3.2]          |[7.5]              |[0.32731750905529733] |\n",
      "|5.0              |3.5             |1.6              |0.6             |0     |[3.5]          |[8.125]            |[1.0156019907136333]  |\n",
      "|5.1              |3.8             |1.9              |0.4             |0     |[3.8]          |[8.75]             |[1.703886472371969]   |\n",
      "|4.8              |3.0             |1.4              |0.3             |0     |[3.0]          |[7.083333333333333]|[-0.13153881205026063]|\n",
      "|5.1              |3.8             |1.6              |0.2             |0     |[3.8]          |[8.75]             |[1.703886472371969]   |\n",
      "|4.6              |3.2             |1.4              |0.2             |0     |[3.2]          |[7.5]              |[0.32731750905529733] |\n",
      "|5.3              |3.7             |1.5              |0.2             |0     |[3.7]          |[8.541666666666666]|[1.4744583118191912]  |\n",
      "|5.0              |3.3             |1.4              |0.2             |0     |[3.3]          |[7.708333333333332]|[0.5567456696080753]  |\n",
      "|7.0              |3.2             |4.7              |1.4             |1     |[3.2]          |[7.5]              |[0.32731750905529733] |\n",
      "|6.4              |3.2             |4.5              |1.5             |1     |[3.2]          |[7.5]              |[0.32731750905529733] |\n",
      "|6.9              |3.1             |4.9              |1.5             |1     |[3.1]          |[7.291666666666666]|[0.09788934850251835] |\n",
      "|5.5              |2.3             |4.0              |1.3             |1     |[2.3]          |[5.625]            |[-1.7375359359197124] |\n",
      "|6.5              |2.8             |4.6              |1.5             |1     |[2.8]          |[6.666666666666666]|[-0.5903951331558186] |\n",
      "|5.7              |2.8             |4.5              |1.3             |1     |[2.8]          |[6.666666666666666]|[-0.5903951331558186] |\n",
      "|6.3              |3.3             |4.7              |1.6             |1     |[3.3]          |[7.708333333333332]|[0.5567456696080753]  |\n",
      "+-----------------+----------------+-----------------+----------------+------+---------------+-------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler(\n",
    "    inputCol=\"sepal_width_vec\",\n",
    "    outputCol=\"sepal_width_std\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "std_model = std_scaler.fit(scaled_df)\n",
    "std_df = std_model.transform(scaled_df)\n",
    "std_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cd7aadf-f794-4e62-b1ef-c03e000aa3ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Resumen / Conclusión\n",
    "\n",
    "En este notebool se ve un montón de cosas que siempre terminamos haciendo al trabajar con datos en Spark y en big data:\n",
    "\n",
    "\n",
    "Encontramos y arreglamos nulos\n",
    "\n",
    "Borramos duplicados y columnas que estorbaban\n",
    "\n",
    "Detectamos valores fuera de lo normal\n",
    "\n",
    "Reorganizamos datos con pivot\n",
    "\n",
    "Expandimos listas con explode\n",
    "\n",
    "Normalizamos y estandarizamos para preparación de modelos\n",
    "\n",
    "Todo esto te deja los datos limpios, ordenados y listos para análisis o machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02 - ML_Preprocesing",
   "notebookOrigID": 99556219460359,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
